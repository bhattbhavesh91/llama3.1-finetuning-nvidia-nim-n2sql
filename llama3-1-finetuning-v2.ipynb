{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8308937-be24-491d-ba57-966469d234aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# wget https://raw.githubusercontent.com/brevdev/notebooks/main/assets/setup-ngc.sh -O setup-ngc\n",
    "# chmod +x setup-ngc\n",
    "# ./setup-ngc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e6f65-a0bb-478f-8bcd-6e682e8991ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should the .nemo checkpoint that is saved\n",
    "!ls ./llama-3_1-8b-instruct-nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3278f02-d298-46ec-b2b3-b89dbbc64d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9142d30-fe8c-4776-ba68-dabaccd024cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80426d64-921b-49f7-b43a-6a57d226b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "169d0c69-02c4-4440-9cb7-70936caba1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70365, Validation size: 354, Test size: 7858\n"
     ]
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.005, random_state=42)\n",
    "\n",
    "# Convert back to Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "343fc77c-7c1e-434b-aa42-b7aaae206771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'SELECT ground FROM table_name_83 WHERE competition = \"tim trophy\" AND time = \"23:00 cet\"', 'question': 'What is the ground of the tim trophy competition, which had a time of 23:00 cet?', 'context': 'CREATE TABLE table_name_83 (ground VARCHAR, competition VARCHAR, time VARCHAR)', '__index_level_0__': 68005}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4768f3e2-8786-41d9-8b87-4b8798307031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output(data):\n",
    "\n",
    "    data_json = PROMPT =f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\"\"\"\n",
    "    \n",
    "    new_data = {\n",
    "                \"input\": f'''{PROMPT} \\nQUESTION: {data[\"question\"]} \\nCONTEXT: {data[\"context\"]}''',\n",
    "                \"output\": data['answer']\n",
    "            }\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ff2561c-e109-4163-afd5-d6f7fcd7c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl_files(input_dataset, output_file_name):\n",
    "    for index in range(input_dataset.num_rows):\n",
    "        new_data = generate_input_output(input_dataset[index])\n",
    "        with open(output_file_name, 'a') as outfile:\n",
    "            json.dump(new_data, outfile)\n",
    "            outfile.write('\\n')\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2669255-4dbe-44ba-86e1-89a61296680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_jsonl_files(train_dataset, 'train_dataset_preprocessed.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a4ba3a7-a5ca-49a3-95bb-d981ddac0643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_jsonl_files(test_dataset, 'test_dataset_preprocessed.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "640a742a-d71a-493f-ae4a-eb40dd3ff9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_jsonl_files(val_dataset, 'val_dataset_preprocessed.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b63a01d-390b-4747-9810-03099c3e2121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-28 16:08:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:08:34 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-07-28 16:08:34 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 50\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 0.2\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: ./results/Meta-llama3.1-8B-Instruct-titlegen\n",
      "      exp_dir: ./results/Meta-llama3.1-8B-Instruct-titlegen\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 32\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: ./llama-3_1-8b-instruct-nemo_v1.0/llama3_1_8b_instruct.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - train_dataset_preprocessed.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - val_dataset_preprocessed.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-28 16:08:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:08:34 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo E 2024-07-28 16:08:34 exp_manager:703] exp_manager received explicit_log_dir: ./results/Meta-llama3.1-8B-Instruct-titlegen and at least one of exp_dir: ./results/Meta-llama3.1-8B-Instruct-titlegen, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2024-07-28 16:08:34 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:08:34 exp_manager:396] Experiments will be logged at results/Meta-llama3.1-8B-Instruct-titlegen\n",
      "[NeMo I 2024-07-28 16:08:34 exp_manager:856] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-28 16:08:34 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 50. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:08:51 megatron_init:263] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:269] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:274] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:277] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:285] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:288] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:289] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:296] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:297] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:306] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:310] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:311] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:331] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:343] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:349] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:350] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:351] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-07-28 16:08:51 megatron_init:352] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-07-28 16:08:51 tokenizer_utils:178] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-07-28 16:08:51 - PID:28506 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:08:52 megatron_base_model:584] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:498] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-07-28 16:08:52 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: rotary_percent in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:09:12 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n",
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "Loading distributed checkpoint directly on the GPU\n",
      "[NeMo I 2024-07-28 16:09:57 nlp_overrides:1180] Model MegatronGPTSFTModel was successfully restored from /root/verb-workspace/llama-3_1-8b-instruct-nemo_v1.0/llama3_1_8b_instruct.nemo.\n",
      "[NeMo I 2024-07-28 16:09:57 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2024-07-28 16:09:57 nlp_adapter_mixins:203] Before adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "[NeMo I 2024-07-28 16:10:01 nlp_adapter_mixins:208] After adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-28 16:10:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-07-28 16:10:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:10:01 megatron_gpt_sft_model:811] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:495] Building indexing for fn = val_dataset_preprocessed.jsonl\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:507] Saving idx file = val_dataset_preprocessed.jsonl.idx.npy\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:509] Saving metadata file = val_dataset_preprocessed.jsonl.idx.info\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.078009\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.069120\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:249] Loading val_dataset_preprocessed.jsonl\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001675\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-07-28 16:10:01 megatron_gpt_sft_model:815] Length of val dataset: 354\n",
      "[NeMo I 2024-07-28 16:10:01 megatron_gpt_sft_model:822] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:495] Building indexing for fn = train_dataset_preprocessed.jsonl\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:507] Saving idx file = train_dataset_preprocessed.jsonl.idx.npy\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:509] Saving metadata file = train_dataset_preprocessed.jsonl.idx.info\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.104657\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.070901\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:249] Loading train_dataset_preprocessed.jsonl\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001006\n",
      "[NeMo I 2024-07-28 16:10:01 text_memmap_dataset:165] Computing global indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-28 16:10:01 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-07-28 16:10:01 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.05 (sec)\n",
      "[NeMo I 2024-07-28 16:10:01 megatron_gpt_sft_model:824] Length of train dataset: 1608\n",
      "[NeMo I 2024-07-28 16:10:01 megatron_gpt_sft_model:829] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-07-28 16:10:01 megatron_gpt_sft_model:829] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2024-07-28 16:10:01 megatron_base_model:1199] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-07-28 16:10:01 nlp_adapter_mixins:269] Optimizer groups set:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "[NeMo I 2024-07-28 16:10:01 modelPT:770] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-07-28 16:10:01 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f79307c6b60>\" \n",
      "    will be used during training (effective maximum steps = 50) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 50\n",
      "    )\n",
      "[NeMo I 2024-07-28 16:10:01 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f79307d8310>\" \n",
      "    will be used during training (effective maximum steps = 50) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 50\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | Float16Module | 8.0 B  | train\n",
      "------------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "8.0 B     Non-trainable params\n",
      "8.0 B     Total params\n",
      "32,162.988Total estimated model params size (MB)\n",
      "[NeMo W 2024-07-28 16:10:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-07-28 16:10:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-07-28 16:10:02 nemo_logging:349] /opt/apex/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "[NeMo W 2024-07-28 16:10:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-07-28 16:10:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-07-28 16:10:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-07-28 16:10:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-07-28 16:10:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  20%|██        | 10/50 [00:57<03:51, reduced_train_loss=1.060, global_step=9.000, consumed_samples=320.0, train_step_timing in s=5.670]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/12 [00:03<00:35,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 2/12 [00:06<00:31,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 3/12 [00:09<00:28,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 4/12 [00:12<00:25,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 5/12 [00:16<00:22,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 6/12 [00:19<00:19,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 7/12 [00:22<00:15,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 8/12 [00:25<00:12,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 9/12 [00:28<00:09,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 10/12 [00:31<00:06,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 11/12 [00:35<00:03,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 12/12 [00:38<00:00,  0.31it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.990\n",
      "Epoch 0, global step 10: 'validation_loss' reached 0.98985 (best 0.98985), saving model to '/root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.990-step=10-consumed_samples=320.0.ckpt' as top 1\n",
      "[NeMo W 2024-07-28 16:11:45 nlp_overrides:480] DistributedCheckpointIO configured but should not be used. Reverting back to TorchCheckpointIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  40%|████      | 20/50 [02:33<03:50, reduced_train_loss=0.422, global_step=19.00, consumed_samples=640.0, train_step_timing in s=5.640, val_loss=0.990]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/12 [00:03<00:34,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 2/12 [00:06<00:31,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 3/12 [00:09<00:28,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 4/12 [00:12<00:25,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 5/12 [00:15<00:21,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 6/12 [00:18<00:18,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 7/12 [00:21<00:15,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 8/12 [00:25<00:12,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 9/12 [00:28<00:09,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 10/12 [00:31<00:06,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 11/12 [00:34<00:03,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 12/12 [00:37<00:00,  0.32it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.659 >= min_delta = 0.001. New best score: 0.331\n",
      "Epoch 0, global step 20: 'validation_loss' reached 0.33098 (best 0.33098), saving model to '/root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.331-step=20-consumed_samples=640.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  40%|████      | 20/50 [03:11<04:46, reduced_train_loss=0.422, global_step=19.00, consumed_samples=640.0, train_step_timing in s=5.640, val_loss=0.331][NeMo I 2024-07-28 16:13:20 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.990-step=10-consumed_samples=320.0.ckpt\n",
      "[NeMo I 2024-07-28 16:13:21 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.990-step=10-consumed_samples=320.0-last.ckpt\n",
      "Epoch 0: :  60%|██████    | 30/50 [04:08<02:45, reduced_train_loss=0.207, global_step=29.00, consumed_samples=960.0, train_step_timing in s=5.670, val_loss=0.331] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/12 [00:03<00:34,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 2/12 [00:06<00:31,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 3/12 [00:09<00:28,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 4/12 [00:12<00:25,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 5/12 [00:15<00:22,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 6/12 [00:18<00:18,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 7/12 [00:22<00:15,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 8/12 [00:25<00:12,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 9/12 [00:28<00:09,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 10/12 [00:31<00:06,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 11/12 [00:34<00:03,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 12/12 [00:37<00:00,  0.32it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.214 >= min_delta = 0.001. New best score: 0.117\n",
      "Epoch 0, global step 30: 'validation_loss' reached 0.11709 (best 0.11709), saving model to '/root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.117-step=30-consumed_samples=960.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  60%|██████    | 30/50 [04:46<03:11, reduced_train_loss=0.207, global_step=29.00, consumed_samples=960.0, train_step_timing in s=5.670, val_loss=0.117][NeMo I 2024-07-28 16:14:56 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.331-step=20-consumed_samples=640.0.ckpt\n",
      "[NeMo I 2024-07-28 16:14:56 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.331-step=20-consumed_samples=640.0-last.ckpt\n",
      "Epoch 0: :  80%|████████  | 40/50 [05:44<01:26, reduced_train_loss=0.063, global_step=39.00, consumed_samples=1280.0, train_step_timing in s=5.780, val_loss=0.117] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/12 [00:03<00:34,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 2/12 [00:06<00:32,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 3/12 [00:09<00:28,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 4/12 [00:12<00:25,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 5/12 [00:15<00:22,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 6/12 [00:18<00:18,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 7/12 [00:21<00:15,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 8/12 [00:25<00:12,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 9/12 [00:28<00:09,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 10/12 [00:31<00:06,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 11/12 [00:34<00:03,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 12/12 [00:37<00:00,  0.32it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.027 >= min_delta = 0.001. New best score: 0.090\n",
      "Epoch 0, global step 40: 'validation_loss' reached 0.08977 (best 0.08977), saving model to '/root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.090-step=40-consumed_samples=1280.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  80%|████████  | 40/50 [06:22<01:35, reduced_train_loss=0.063, global_step=39.00, consumed_samples=1280.0, train_step_timing in s=5.780, val_loss=0.0898][NeMo I 2024-07-28 16:16:32 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.117-step=30-consumed_samples=960.0.ckpt\n",
      "[NeMo I 2024-07-28 16:16:32 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.117-step=30-consumed_samples=960.0-last.ckpt\n",
      "Epoch 0: : 100%|██████████| 50/50 [07:20<00:00, reduced_train_loss=0.0555, global_step=49.00, consumed_samples=1600.0, train_step_timing in s=5.660, val_loss=0.0898]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/12 [00:03<00:34,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 2/12 [00:06<00:31,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 3/12 [00:09<00:28,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 4/12 [00:12<00:25,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 5/12 [00:15<00:22,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 6/12 [00:19<00:19,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 7/12 [00:22<00:15,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 8/12 [00:25<00:12,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 9/12 [00:28<00:09,  0.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 10/12 [00:31<00:06,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 11/12 [00:34<00:03,  0.32it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 12/12 [00:37<00:00,  0.32it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.001. New best score: 0.074\n",
      "Epoch 0, global step 50: 'validation_loss' reached 0.07400 (best 0.07400), saving model to '/root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.074-step=50-consumed_samples=1600.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 50/50 [07:58<00:00, reduced_train_loss=0.0555, global_step=49.00, consumed_samples=1600.0, train_step_timing in s=5.660, val_loss=0.074] [NeMo I 2024-07-28 16:18:08 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.090-step=40-consumed_samples=1280.0.ckpt\n",
      "[NeMo I 2024-07-28 16:18:08 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.090-step=40-consumed_samples=1280.0-last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 50/50 [07:59<00:00, reduced_train_loss=0.0555, global_step=49.00, consumed_samples=1600.0, train_step_timing in s=5.660, val_loss=0.074]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.074-step=50-consumed_samples=1600.0.ckpt\n",
      "Restored all states from the checkpoint at /root/verb-workspace/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.074-step=50-consumed_samples=1600.0.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "MODEL=\"./llama-3_1-8b-instruct-nemo_v1.0/llama3_1_8b_instruct.nemo\"\n",
    "\n",
    "TRAIN_DS=\"[train_dataset_preprocessed.jsonl]\"\n",
    "VALID_DS=\"[val_dataset_preprocessed.jsonl]\"\n",
    "TEST_DS=\"[test_dataset_preprocessed.jsonl]\"\n",
    "TEST_NAMES=\"[n2sql]\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "rm -rf results\n",
    "OUTPUT_DIR=\"./results/Meta-llama3.1-8B-Instruct-titlegen\"\n",
    "\n",
    "torchrun --nproc_per_node=1 \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${OUTPUT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    trainer.val_check_interval=0.2 \\\n",
    "    trainer.max_steps=50 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=32 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=${SCHEME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ae1036d-d8c3-4cc8-ac05-a262930f1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 307504\n",
      "-rw-r--r-- 1 root root 146928174 Jul 28 16:18 'megatron_gpt_peft_lora_tuning--validation_loss=0.074-step=50-consumed_samples=1600.0-last.ckpt'\n",
      "-rw-r--r-- 1 root root 146928174 Jul 28 16:18 'megatron_gpt_peft_lora_tuning--validation_loss=0.074-step=50-consumed_samples=1600.0.ckpt'\n",
      "-rw-r--r-- 1 root root  21012480 Jul 28 16:18  megatron_gpt_peft_lora_tuning.nemo\n"
     ]
    }
   ],
   "source": [
    " # Check that the LORA model file exists\n",
    "!ls -l ./results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93626da2-cd4a-44f6-ade5-8bb4fd332671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
